{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Prompts with **Automatic Prompt Engineer** (APE)\n",
    "\n",
    "This notebook demonstrates how to use Automatic Prompt Engineer (APE) (arxiv link) to optimize prompts for text generation. In its simplest form, APE takes as input a dataset (a list of inputs and a list of outputs), a prompt template, and optimizes this prompt template so that it generates the outputs given the inputs.\n",
    "\n",
    "APE accomplishes this in two steps. First, it uses a language model to generate a set of candidate prompts. Then, it uses a prompt evaluation function to evaluate the quality of each candidate prompt. Finally, it returns the prompt with the highest evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT_forward] Generating 4 completions, split into 1 batches of size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " take the letter, convert it to its corresponding number in the alphabet, and then multiply it by 2. For example, e is the 5th letter in the alphabet, so when multiplied by 2 it becomes 10. However, since\n",
      " assign each letter of the alphabet a numerical value based on its position in the alphabet. So, for example, \"a\" would have a value of 1, \"b\" would have a value of 2, and so on. In this\n",
      " assign a numerical value to each letter of the alphabet, starting with A=1 and ending with Z=26. In this case, the input corresponds to the letter's position in the alphabet, with A=1, B=2, C=\n",
      " assign a numerical value to each letter of the alphabet, starting with \"a\" being equal to 1 and increasing by 1 for each subsequent letter. Therefore, \"b\" would be equal to 2, \"c\" to 3,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from automatic_prompt_engineer import generate, config, template\n",
    "\n",
    "def test_generate_instruction():\n",
    "    prompt_gen_template = \"I gave a friend an instruction. Based on the instruction they produced the following input-output pairs:\\n\\n[full_DEMO]\\n\\nThe instruction was to [APE]\"\n",
    "    demos_template = \"Input: [INPUT]\\nOutput: [OUTPUT]\"\n",
    "\n",
    "    prompt_gen_template = template.GenerationTemplate(prompt_gen_template)\n",
    "    demos_template = template.DemosTemplate(demos_template)\n",
    "\n",
    "    inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    outputs = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    data = inputs, outputs\n",
    "\n",
    "    user_config = {\n",
    "        'generation': {\n",
    "            'num_subsamples': 2,\n",
    "            'num_demos': 5,\n",
    "            'num_prompts_per_subsample': 2,\n",
    "            'model': {\n",
    "                'gpt_config': {\n",
    "                    'model': 'gpt-35-turbo-instruct'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    conf = config.update_config(user_config)\n",
    "\n",
    "    prompts = generate.generate_prompts(\n",
    "        prompt_gen_template, demos_template, data, conf['generation'])\n",
    "\n",
    "    for p in prompts:\n",
    "        print(p)\n",
    "    assert len(prompts) == 4\n",
    "\n",
    "test_generate_instruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a simple dataset consisting of words and their antonyms.\n",
    "words = [\"sane\", \"direct\", \"informally\", \"unpopular\", \"subtractive\", \"nonresidential\",\n",
    "    \"inexact\", \"uptown\", \"incomparable\", \"powerful\", \"gaseous\", \"evenly\", \"formality\",\n",
    "    \"deliberately\", \"off\"]\n",
    "antonyms = [\"insane\", \"indirect\", \"formally\", \"popular\", \"additive\", \"residential\",\n",
    "    \"exact\", \"downtown\", \"comparable\", \"powerless\", \"solid\", \"unevenly\", \"informality\",\n",
    "    \"accidentally\", \"on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to define the format of the prompt that we are using.\n",
    "\n",
    "eval_template = \\\n",
    "\"\"\"Instruction: [PROMPT]\n",
    "Input: [INPUT]\n",
    "Output: [OUTPUT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts...\n",
      "[GPT_forward] Generating 50 completions, split into 1 batches of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned 50 prompts. Deduplicating...\n",
      "Deduplicated to 37 prompts.\n",
      "Evaluating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, let's use APE to find prompts that generate antonyms for each word.\n",
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "result, demo_fn = ape.simple_ape(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: prompt\n",
      "----------------\n",
      "-0.10:  create an opposite or contrasting word.\n",
      "-0.13:  find the opposite or inverse of each word. \n",
      "-0.14:  find the opposite or inverse of the given word.\n",
      "-0.14:  change the word by making it an antonym.\n",
      "-0.15:  find the opposite or contrasting word for each given word.\n",
      "-0.15:  find the opposite or antonym of each given word.\n",
      "-0.15:  create an antonym, or a word that has the opposite meaning, of the given input.\n",
      "-0.17:  create an opposite or contrasting word to the given input.\n",
      "-0.17:  create an opposite or contrasting word for each input.\n",
      "-0.17:  change the word to its opposite or antonym.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the results.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with a prompt written by a human:\n",
    "\n",
    "\"*Write an antonym to the following word.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "manual_prompt = \"Write an antonym to the following word.\"\n",
    "\n",
    "human_result = ape.simple_eval(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    "    prompts=[manual_prompt],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(p): prompt\n",
      "----------------\n",
      "-0.59: Write an antonym to the following word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(human_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2afcb7a2e6fcb9490d448e607abf9226c3f7acca28baeea9bc24b456562037f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
